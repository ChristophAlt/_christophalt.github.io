<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Christoph Alt</title>
    <link>https://christophalt.github.io/post/</link>
      <atom:link href="https://christophalt.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 27 Aug 2019 20:19:38 +0200</lastBuildDate>
    <image>
      <url>https://christophalt.github.io/img/icon-192.png</url>
      <title>Posts</title>
      <link>https://christophalt.github.io/post/</link>
    </image>
    
    <item>
      <title>ACL 2019 Highlights: The Year of the BERT</title>
      <link>https://christophalt.github.io/post/acl-2019-highlights/</link>
      <pubDate>Tue, 27 Aug 2019 20:19:38 +0200</pubDate>
      <guid>https://christophalt.github.io/post/acl-2019-highlights/</guid>
      <description>

&lt;p&gt;I attended the &lt;a href=&#34;http://www.acl2019.org&#34; target=&#34;_blank&#34;&gt;57th Annual Meeting of the Association for Computational Linguistics (ACL 2019)&lt;/a&gt; in Florence, Italy from July 28th to August 2nd to present our paper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and attend the main conference, &lt;a href=&#34;https://blackboxnlp.github.io&#34; target=&#34;_blank&#34;&gt;BlackboxNLP workshop&lt;/a&gt;, and &lt;a href=&#34;https://sites.google.com/view/repl4nlp2019&#34; target=&#34;_blank&#34;&gt;Rep4NLP workshop&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Lead with the record number of submissions, &lt;a href=&#34;http://acl2019pcblog.fileli.unipi.it/?p=156&#34; target=&#34;_blank&#34;&gt;http://acl2019pcblog.fileli.unipi.it/?p=156&lt;/a&gt;, recap last year with the main theme understanding representations and understanding state of the art models (of course, BERT)&lt;/p&gt;

&lt;h1 id=&#34;understanding-contextualized-word-representations&#34;&gt;Understanding Contextualized Word Representations&lt;/h1&gt;

&lt;h1 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h1&gt;

&lt;p&gt;Gorman, K., &amp;amp; Bedrick, S. (2019, July). We need to talk about standard splits.&lt;/p&gt;

&lt;p&gt;Dror, R., Shlomov, S., &amp;amp; Reichart, R. (2019, July). Deep Dominance-How to Properly Compare Deep Neural Models.&lt;/p&gt;

&lt;h1 id=&#34;understanding-bert&#34;&gt;Understanding BERT&lt;/h1&gt;

&lt;p&gt;Clark, K., Khandelwal, U., Levy, O., &amp;amp; Manning, C. D. (2019). What Does BERT Look At? An Analysis of BERT&amp;rsquo;s Attention.&lt;/p&gt;

&lt;p&gt;Mareček, D., &amp;amp; Rosa, R. (2019). From Balustrades to Pierre Vinken: Looking for Syntax in Transformer Self-Attentions.&lt;/p&gt;

&lt;p&gt;[^2]:&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Alt, C., Hübner, M., &amp;amp; Hennig, L. (2019). Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction. In Proceedings of ACL 2019.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Neural Relation Extraction Part 1</title>
      <link>https://christophalt.github.io/post/neural-relation-extraction-part-1/</link>
      <pubDate>Tue, 27 Aug 2019 20:04:21 +0200</pubDate>
      <guid>https://christophalt.github.io/post/neural-relation-extraction-part-1/</guid>
      <description>&lt;h1 id=&#34;test-1&#34;&gt;Test 1&lt;/h1&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\gamma_{n} = \frac{ 
\left | \left (\mathbf x_{n} - \mathbf x_{n-1} \right )^T 
\left [\nabla F (\mathbf x_{n}) - \nabla F (\mathbf x_{n-1}) \right ] \right |}
{\left \|\nabla F(\mathbf{x}_{n}) - \nabla F(\mathbf{x}_{n-1}) \right \|^2}\]&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&#34;test-2&#34;&gt;Test 2&lt;/h1&gt;

&lt;p&gt;I have more &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; to say.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Footnote example.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
