[{"authors":["admin"],"categories":null,"content":"I am a researcher / software engineer with 4+ years of experience (research and applied) in machine learning \u0026amp; natural language processing \u0026amp; distributed computing. Fluent in Python, Java, Python\u0026rsquo;s scientific and machine learning stack, recent deep learning and natural language processing frameworks, Docker, and AWS. I am passionate about solving difficult problems with little data.\nCurrently, I am a researcher and PhD student at the German Research Center for AI (DFKI), Berlin. My goal is to solve complex natural language understanding problems with limited (labeled) data, e.g., via transfer-learning, multi-task learning, few- and zero-shot learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://christophalt.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a researcher / software engineer with 4+ years of experience (research and applied) in machine learning \u0026amp; natural language processing \u0026amp; distributed computing. Fluent in Python, Java, Python\u0026rsquo;s scientific and machine learning stack, recent deep learning and natural language processing frameworks, Docker, and AWS. I am passionate about solving difficult problems with little data.\nCurrently, I am a researcher and PhD student at the German Research Center for AI (DFKI), Berlin.","tags":null,"title":"","type":"authors"},{"authors":["Christoph Alt","Aleksandra Gabryszak","Leonhard Hennig"],"categories":[],"content":"","date":1592532000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592532000,"objectID":"410e295227c53234c99f5885bd7bda07","permalink":"https://christophalt.github.io/publication/tacred-revisited/","publishdate":"2020-05-13T19:09:20+02:00","relpermalink":"/publication/tacred-revisited/","section":"publication","summary":"TACRED is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pre-training and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate?","tags":[],"title":"TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task","type":"publication"},{"authors":["Christoph Alt","Aleksandra Gabryszak","Leonhard Hennig"],"categories":[],"content":"","date":1592528400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592528400,"objectID":"356c202a7a495c323b56244cdfaa64af","permalink":"https://christophalt.github.io/publication/probing-linguistic/","publishdate":"2020-05-13T19:09:15+02:00","relpermalink":"/publication/probing-linguistic/","section":"publication","summary":"Despite the recent progress, little is known about the features captured by state-of-the-art neural relation extraction (RE) models. Common methods encode the source sentence, conditioned on the entity mentions, before classifying the relation. However, the complexity of the task makes it difficult to understand how encoder architecture and supporting linguistic knowledge affect the features learned by the encoder. We introduce 14 probing tasks targeting linguistic properties relevant to RE, and we use them to study representations learned by more than 40 different encoder architecture and linguistic feature combinations trained on two datasets, TACRED and SemEval 2010 Task 8.","tags":[],"title":"Probing Linguistic Features of Sentence-level Representations in Neural Relation Extraction","type":"publication"},{"authors":["Hanchu Zhang","Leonhard Hennig","Christoph Alt","Changjian Hu","Yao Meng","Chao Wang"],"categories":[],"content":"","date":1592524800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592524800,"objectID":"9cdf3935ad034e5570e2b6337ec40f47","permalink":"https://christophalt.github.io/publication/bootstrapping-ner/","publishdate":"2020-05-13T19:09:55+02:00","relpermalink":"/publication/bootstrapping-ner/","section":"publication","summary":"In this work, we introduce a bootstrapped, iterative NER model that integrates a PU learning algorithm for recognizing named entities in a low-resource setting. Our approach combines dictionary-based labeling with syntactically-informed label expansion to efficiently enrich the seed dictionaries. Experimental results on a dataset of manually annotated e-commerce product descriptions demonstrate the effectiveness of the proposed framework.","tags":[],"title":"Bootstrapping Named Entity Recognition in E-Commerce with Positive Unlabeled Learning","type":"publication"},{"authors":["David Harbecke","Christoph Alt"],"categories":[],"content":"","date":1592524800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592524800,"objectID":"da39e7483354b96828aefb83289d9ed0","permalink":"https://christophalt.github.io/publication/considering-likelihood/","publishdate":"2020-05-13T19:09:55+02:00","relpermalink":"/publication/considering-likelihood/","section":"publication","summary":"Recently, state-of-the-art NLP models gained an increasing syntactic and semantic understanding of language, and explanation methods are crucial to understand their decisions. Occlusion is a well established method that provides explanations on discrete language data, e.g. by removing a language unit from an input and measuring the impact on a model's decision. We argue that current occlusion-based methods often produce invalid or syntactically incorrect language data, neglecting the improved abilities of recent NLP models. Furthermore, gradient-based explanation methods disregard the discrete distribution of data in NLP. Thus, we propose OLM: a novel explanation method that combines occlusion and language models to sample valid and syntactically correct replacements with high likelihood, given the context of the original input. We lay out a theoretical foundation that alleviates these weaknesses of other explanation methods in NLP and provide results that underline the importance of considering data likelihood in occlusion-based explanation.","tags":[],"title":"Considering Likelihood in NLP Classification Explanations with Occlusion and Language Modeling","type":"publication"},{"authors":["Robert Schwarzenberg","Marc Hübner","David Harbecke","Christoph Alt","Leonhard Hennig"],"categories":[],"content":"","date":1569476323,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569476323,"objectID":"3046496158305b5930b85c22a137148d","permalink":"https://christophalt.github.io/publication/layerwise-relevance/","publishdate":"2019-09-26T07:38:43+02:00","relpermalink":"/publication/layerwise-relevance/","section":"publication","summary":"Representations in the hidden layers of Deep Neural Networks (DNN) are often hard to interpret since it is difficult to project them into an interpretable domain. Graph Convolutional Networks (GCN) allow this projection, but existing explainability methods do not exploit this fact, i.e. do not focus their explanations on intermediate states. In this work, we present a novel method that traces and visualizes features that contribute to a classification decision in the visible and hidden layers of a GCN. Our method exposes hidden cross-layer dynamics in the input graph structure. We experimentally demonstrate that it yields meaningful layerwise explanations for a GCN sentence classifier.","tags":[],"title":"Layerwise Relevance Visualization in Convolutional Text Graph Classifiers","type":"publication"},{"authors":["Christoph Alt","Marc Hübner","Leonhard Hennig"],"categories":[],"content":"","date":1564358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564358400,"objectID":"e81f6dcb47ba4246eadfaeeefef05f9b","permalink":"https://christophalt.github.io/publication/fine-tuning/","publishdate":"2019-08-26T14:09:16+02:00","relpermalink":"/publication/fine-tuning/","section":"publication","summary":"We show that generative language model pre-training combined with selective attention improves recall for long-tail relations in distantly supervised neural relation extraction.","tags":[],"title":"Fine-Tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction","type":"publication"},{"authors":["Christoph Alt","Marc Hübner","Leonhard Hennig"],"categories":[],"content":"","date":1558310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558310400,"objectID":"1ee44b0a7f9c1313e0f341180a18cd8b","permalink":"https://christophalt.github.io/publication/improving-relation/","publishdate":"2019-08-26T14:36:10+02:00","relpermalink":"/publication/improving-relation/","section":"publication","summary":"We show that transfer learning through generative language model pre-training improves supervised neural relation extraction, achieving new state-of-the-art performance on TACRED and SemEval 2010 Task 8.","tags":[],"title":"Improving Relation Extraction by Pre-Trained Language Representations","type":"publication"},{"authors":["Roland Roller","Christoph Alt","Laura Seiffe","He Wang"],"categories":[],"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"f1a111cc08757da0239462291892fc51","permalink":"https://christophalt.github.io/publication/mex/","publishdate":"2019-08-26T14:36:19+02:00","relpermalink":"/publication/mex/","section":"publication","summary":"","tags":[],"title":"mEx - an Information Extraction Platform for German Medical Text","type":"publication"},{"authors":["David Harbecke","Robert Schwarzenberg","Christoph Alt"],"categories":[],"content":"","date":1541116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541116800,"objectID":"71faa062a3b60a41d1ed2b9016d30f4b","permalink":"https://christophalt.github.io/publication/learning-explanations/","publishdate":"2019-08-26T14:36:16+02:00","relpermalink":"/publication/learning-explanations/","section":"publication","summary":"PatternAttribution is a recent method, introduced in the vision domain, that explains classifications of deep neural networks. We demonstrate that it also generates meaningful interpretations in the language domain.","tags":[],"title":"Learning Explanations From Language Data","type":"publication"},{"authors":null,"categories":null,"content":"\r[28/06/2020] I submitted my PhD thesis titled \u0026ldquo;Neural Sequential Transfer Learning for Relation Extraction\u0026rdquo;.\n[12/05/2020] Our paper \u0026ldquo;Bootstrapping Named Entity Recognition in E-Commerce with Positive Unlabeled Learning\u0026rdquo; got accepted at ECNLP @ ACL 2020.\n[17/04/2020] Our paper \u0026ldquo;Considering Likelihood in NLP Classification Explanations with Occlusion and Language Modeling\u0026rdquo; got accepted at ACL 2020 Student Research Workshop.\n[04/04/2020] Our paper \u0026ldquo;TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task\u0026rdquo; got accepted at ACL 2020.\n[04/04/2020] Our paper \u0026ldquo;Probing Linguistic Features of Sentence-level Representations in Neural Relation Extraction\u0026rdquo; got accepted at ACL 2020.\n","date":1512082800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512082800,"objectID":"a0812ae5f3c926fea6faf4472cefc8e2","permalink":"https://christophalt.github.io/news/","publishdate":"2017-12-01T00:00:00+01:00","relpermalink":"/news/","section":"","summary":"\r\nList of news.\r\n","tags":[],"title":"News","type":"page"}]