[{"authors":["admin"],"categories":null,"content":"I am a PhD student and researcher in the natural language understanding (NLU) research group of the German Research Center for AI\u0026rsquo;s Speech and Language Technology lab in Berlin. My main research interest include sample-efficient information extraction, including transfer-learning, multi-task learning, and few-shot learning.\n","date":1566929061,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1566929061,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://christophalt.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD student and researcher in the natural language understanding (NLU) research group of the German Research Center for AI\u0026rsquo;s Speech and Language Technology lab in Berlin. My main research interest include sample-efficient information extraction, including transfer-learning, multi-task learning, and few-shot learning.","tags":null,"title":"Christoph Alt","type":"authors"},{"authors":["Robert Schwarzenberg","Marc Hübner","David Harbecke","Christoph Alt","Leonhard Hennig"],"categories":[],"content":"","date":1569476323,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569476323,"objectID":"a473d0f5238b0e0755c80265a795c54a","permalink":"https://christophalt.github.io/publication/layerwise-relevance-visualization-in-convolutional-text-graph-classifiers/","publishdate":"2019-09-26T07:38:43+02:00","relpermalink":"/publication/layerwise-relevance-visualization-in-convolutional-text-graph-classifiers/","section":"publication","summary":"Representations in the hidden layers of Deep Neural Networks (DNN) are often hard to interpret since it is difficult to project them into an interpretable domain. Graph Convolutional Networks (GCN) allow this projection, but existing explainability methods do not exploit this fact, i.e. do not focus their explanations on intermediate states. In this work, we present a novel method that traces and visualizes features that contribute to a classification decision in the visible and hidden layers of a GCN. Our method exposes hidden cross-layer dynamics in the input graph structure. We experimentally demonstrate that it yields meaningful layerwise explanations for a GCN sentence classifier.","tags":[],"title":"Layerwise Relevance Visualization in Convolutional Text Graph Classifiers","type":"publication"},{"authors":[],"categories":[],"content":" I attended the 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019) in Florence, Italy from July 28th to August 2nd to present our paper1 and attend the main conference, BlackboxNLP workshop, and Rep4NLP workshop.\nLead with the record number of submissions, http://acl2019pcblog.fileli.unipi.it/?p=156, recap last year with the main theme understanding representations and understanding state of the art models (of course, BERT)\nUnderstanding Contextualized Word Representations Evaluation Gorman, K., \u0026amp; Bedrick, S. (2019, July). We need to talk about standard splits.\nDror, R., Shlomov, S., \u0026amp; Reichart, R. (2019, July). Deep Dominance-How to Properly Compare Deep Neural Models.\nUnderstanding BERT Clark, K., Khandelwal, U., Levy, O., \u0026amp; Manning, C. D. (2019). What Does BERT Look At? An Analysis of BERT\u0026rsquo;s Attention.\nMareček, D., \u0026amp; Rosa, R. (2019). From Balustrades to Pierre Vinken: Looking for Syntax in Transformer Self-Attentions.\n[^2]:\n Alt, C., Hübner, M., \u0026amp; Hennig, L. (2019). Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction. In Proceedings of ACL 2019. ^   ","date":1566929978,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566929978,"objectID":"c61fe7c03c2c18b804b410133b71c631","permalink":"https://christophalt.github.io/post/acl-2019-highlights/","publishdate":"2019-08-27T20:19:38+02:00","relpermalink":"/post/acl-2019-highlights/","section":"post","summary":"I attended the 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019) in Florence, Italy from July 28th to August 2nd to present our paper1 and attend the main conference, BlackboxNLP workshop, and Rep4NLP workshop.\nLead with the record number of submissions, http://acl2019pcblog.fileli.unipi.it/?p=156, recap last year with the main theme understanding representations and understanding state of the art models (of course, BERT)\nUnderstanding Contextualized Word Representations Evaluation Gorman, K.","tags":[],"title":"ACL 2019 Highlights: The Year of the BERT","type":"post"},{"authors":["Christoph Alt"],"categories":[],"content":"Test 1 \\[\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}\\]\nTest 2 I have more 1 to say.\n  Footnote example. ^   ","date":1566929061,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566929061,"objectID":"8cb3adab19d57aa67b1a72cbfa30b743","permalink":"https://christophalt.github.io/post/neural-relation-extraction-part-1/","publishdate":"2019-08-27T20:04:21+02:00","relpermalink":"/post/neural-relation-extraction-part-1/","section":"post","summary":"Test 1 \\[\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}\\]\nTest 2 I have more 1 to say.\n  Footnote example. ^   ","tags":[],"title":"Neural Relation Extraction Part 1","type":"post"},{"authors":["Christoph Alt","Marc Hübner","Leonhard Hennig"],"categories":[],"content":"","date":1564358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564358400,"objectID":"f4eb940c053dcbb0e51e8bc6f3c1280c","permalink":"https://christophalt.github.io/publication/fine-tuning-pre-trained-transformer-language-models-to-distantly-supervised-relation-extraction/","publishdate":"2019-08-26T14:09:16+02:00","relpermalink":"/publication/fine-tuning-pre-trained-transformer-language-models-to-distantly-supervised-relation-extraction/","section":"publication","summary":"We show that generative language model pre-training combined with selective attention improves recall for long-tail relations in distantly supervised neural relation extraction.","tags":[],"title":"Fine-Tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction","type":"publication"},{"authors":["Christoph Alt","Marc Hübner","Leonhard Hennig"],"categories":[],"content":"","date":1558310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558310400,"objectID":"ba406529ce40592af9b0e1d9b0ba770f","permalink":"https://christophalt.github.io/publication/improving-relation-extraction-by-pre-trained-language-representations/","publishdate":"2019-08-26T14:36:10+02:00","relpermalink":"/publication/improving-relation-extraction-by-pre-trained-language-representations/","section":"publication","summary":"We show that transfer learning through generative language model pre-training improves supervised neural relation extraction, achieving new state-of-the-art performance on TACRED and SemEval 2010 Task 8.","tags":[],"title":"Improving Relation Extraction by Pre-Trained Language Representations","type":"publication"},{"authors":["Roland Roller","Christoph Alt","Laura Seiffe","He Wang"],"categories":[],"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"433a17498e14b803ea57862a8b6aadd1","permalink":"https://christophalt.github.io/publication/mex-an-information-extraction-platform-for-german-medical-text/","publishdate":"2019-08-26T14:36:19+02:00","relpermalink":"/publication/mex-an-information-extraction-platform-for-german-medical-text/","section":"publication","summary":"","tags":[],"title":"mEx - an Information Extraction Platform for German Medical Text","type":"publication"},{"authors":["David Harbecke","Robert Schwarzenberg","Christoph Alt"],"categories":[],"content":"","date":1541116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541116800,"objectID":"68f68d704af9c42e6dc03edd211ae360","permalink":"https://christophalt.github.io/publication/learning-explanations-from-language-data/","publishdate":"2019-08-26T14:36:16+02:00","relpermalink":"/publication/learning-explanations-from-language-data/","section":"publication","summary":"PatternAttribution is a recent method, introduced in the vision domain, that explains classifications of deep neural networks. We demonstrate that it also generates meaningful interpretations in the language domain.","tags":[],"title":"Learning Explanations From Language Data","type":"publication"}]