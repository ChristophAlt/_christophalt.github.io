[{"authors":["admin"],"categories":null,"content":"I am a PhD student and researcher in the natural language understanding (NLU) research group of the German Research Center for AI\u0026rsquo;s Speech and Language Technology lab in Berlin. My main research interest include sample-efficient information extraction, including transfer-learning, multi-task learning, and few-shot learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://christophalt.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD student and researcher in the natural language understanding (NLU) research group of the German Research Center for AI\u0026rsquo;s Speech and Language Technology lab in Berlin. My main research interest include sample-efficient information extraction, including transfer-learning, multi-task learning, and few-shot learning.","tags":null,"title":"Christoph Alt","type":"authors"},{"authors":["Christoph Alt","Marc Hübner","Leonhard Hennig"],"categories":[],"content":"","date":1564358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564358400,"objectID":"f4eb940c053dcbb0e51e8bc6f3c1280c","permalink":"https://christophalt.github.io/publication/fine-tuning-pre-trained-transformer-language-models-to-distantly-supervised-relation-extraction/","publishdate":"2019-08-26T14:09:16+02:00","relpermalink":"/publication/fine-tuning-pre-trained-transformer-language-models-to-distantly-supervised-relation-extraction/","section":"publication","summary":"We show that generative language model pre-training combined with selective attention improves recall for long-tail relations in distantly supervised neural relation extraction.","tags":[],"title":"Fine-Tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction","type":"publication"},{"authors":["Christoph Alt","Marc Hübner","Leonhard Hennig"],"categories":[],"content":"","date":1558310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558310400,"objectID":"ba406529ce40592af9b0e1d9b0ba770f","permalink":"https://christophalt.github.io/publication/improving-relation-extraction-by-pre-trained-language-representations/","publishdate":"2019-08-26T14:36:10+02:00","relpermalink":"/publication/improving-relation-extraction-by-pre-trained-language-representations/","section":"publication","summary":"We show that transfer learning through generative language model pre-training improves supervised neural relation extraction, achieving new state-of-the-art performance on TACRED and SemEval 2010 Task 8.","tags":[],"title":"Improving Relation Extraction by Pre-Trained Language Representations","type":"publication"},{"authors":["Roland Roller","Christoph Alt","Laura Seiffe","He Wang"],"categories":[],"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"433a17498e14b803ea57862a8b6aadd1","permalink":"https://christophalt.github.io/publication/mex-an-information-extraction-platform-for-german-medical-text/","publishdate":"2019-08-26T14:36:19+02:00","relpermalink":"/publication/mex-an-information-extraction-platform-for-german-medical-text/","section":"publication","summary":"","tags":[],"title":"mEx - an Information Extraction Platform for German Medical Text","type":"publication"},{"authors":["David Harbecke","Robert Schwarzenberg","Christoph Alt"],"categories":[],"content":"","date":1541116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541116800,"objectID":"68f68d704af9c42e6dc03edd211ae360","permalink":"https://christophalt.github.io/publication/learning-explanations-from-language-data/","publishdate":"2019-08-26T14:36:16+02:00","relpermalink":"/publication/learning-explanations-from-language-data/","section":"publication","summary":"PatternAttribution is a recent method, introduced in the vision domain, that explains classifications of deep neural networks. We demonstrate that it also generates meaningful interpretations in the language domain.","tags":[],"title":"Learning Explanations From Language Data","type":"publication"}]